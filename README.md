# DiceGame



A Markov decision process, MDP, is used to model decision-making when there is uncertainty. It is an extension of a Markov chain, where there exist possible starting states, and probabilities for transitioning into these states. In addition to this, an MDP adds actions and rewards. For each state, there are possible actions, each yielding a reward and probability to transition into a new state. In an MDP the future is independent of the past, it only depends on the current state that it is in and calculating the expected reward of each subsequent action. 

A dice game with 3 fair 6 sided dice where at each role the player can either stick and doubles or triples are flipped and the scored summed, or re-roll any combination of the dice but giving a penalty of -1. This can be modelled as an MDP because each roll is independent of the past. It is therefore possible to find the optimal policy for a current state, where a policy is state action pair. A state being a tuple of the dice side and action the optimal action for that state to gain the highest reward. 

Starting with state s and under action a there is a probability p of transitioning into a new state s’ and gaining reward r. In regards to the dice game the states are all possible configurations of three dice, there are 56. Actions, a, are roll dice 1 or 2 or 3 or roll dices 1 and 2 or 1 and 3 or 2 and 3, roll all three dices or hold and end the game. The new states, s’, are those that can occur from a given action and the reward is the sum of the dice as set out in the rules of the game. 

The aim is to find an optimal action for each state. My approach was to create a dictionary with each state action pair, where the action is the optimal for that state. To do this, the algorithm iterates through every possible state, and for every action from this state, finds the possible next states that can be obtained with their reward and probabilities. Using this, the expected reward for each action can be calculated. The action that enables the greatest expected reward is assigned the best_action variable, and added to the dictionary corresponding to that state, this is the policy. However, this is only taking into account the immediate reward, this can be called a greedy policy. Whereas the actual optimal policy will be that that finds the best long-term reward for each action, this is where value iteration is used.

Value iteration involves adding the maximum expected reward of s’ multiplied by gamma to the expected value of state s. Gamma is a discounting factor, a gamma of 1 implies each reward is of equal importance, a gamma of 0 is only concerned with the immediate reward. For the dice algorithm, a value of gamma=1 has been used. This new value of the expected return at s and the discounted expected return at s’ is called the value function and the equation is called the bellman equation. This is a recursive formula, for each iteration using the value function of the s'. To initialise this process the starting value function is 0. Value iteration continues until the difference in one step to the next converges and is less than a predetermined value, theta. In this algorithm theta is 0.1. Once convergence is reached, an optimal policy for each state has been found. In regards to the dice problem each possible dice state has its optimal action, this is the optimal policy. This value iteration algorithm is initialised in the agent. It is therefore stored and so each time the play function is used, the policy dictionary does not have to be recalculated each time. Using the Dice Game agent to call the actions and states enables this code to be used for modifications of the game with differing number of dice and different sides. The play function has an input of the rolled dice and an optimal action as the output. 

My code gives an average score between 11.5 and 14 when tested on 10 games. A comparison between my agent with the always hold agent averages and the perfectionist agent can be made. Setting the same seed over 10 games, my agent has an average of 13.2, the always hold agent 10 and the perfectionist agent -37. From this I can conclude that my agent successfully implements an optimal strategy for the dice game. 

